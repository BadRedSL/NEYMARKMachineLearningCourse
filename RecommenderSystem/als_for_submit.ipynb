{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: implicit in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: seaborn==0.12.0 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn==0.12.0) (1.24.1)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn==0.12.0) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn==0.12.0) (3.6.3)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from implicit) (1.10.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from implicit) (4.64.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alex\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.1->seaborn==0.12.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.25->seaborn==0.12.0) (2022.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->implicit) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.1->seaborn==0.12.0) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U implicit seaborn==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Sequence, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('./neymark-ml-recsys')\n",
    "\n",
    "ratings = pd.read_csv(data_path / 'train_ratings.csv')\n",
    "trainsactions = pd.read_csv(data_path / 'train_transactions.csv')\n",
    "bookmarks = pd.read_csv(data_path / 'train_bookmarks.csv')\n",
    "\n",
    "with open(data_path / 'catalogue.json', 'r') as f:\n",
    "    meta_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks[\"bookmark\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions = trainsactions.merge(bookmarks[[\n",
    "        'element_uid', \n",
    "        'user_uid',\n",
    "        'bookmark',\n",
    "    ]], on=['element_uid', 'user_uid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions.fillna(value={\"bookmark\":0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions['split_rank_per_user'] = (\n",
    "    trainsactions\n",
    "    .groupby('user_uid')['ts']\n",
    "    .rank('first', ascending=False)\n",
    "    .astype('int32')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_to_df(meta_raw):\n",
    "    element_uid = []\n",
    "    duration = []\n",
    "    type_ = []\n",
    "    attributes = []\n",
    "    availability = []\n",
    "    feature_1 = []\n",
    "    feature_2 = []\n",
    "    feature_3 = []\n",
    "    feature_4 = []\n",
    "    feature_5 = []\n",
    "    for k, v in meta_raw.items():\n",
    "        element_uid.append(int(k))\n",
    "        duration.append(float(v['duration']) * 60)\n",
    "        type_.append(v['type'])\n",
    "        attributes.append(v[\"attributes\"])\n",
    "        availability.append(v[\"availability\"])\n",
    "        feature_1.append(v[\"feature_1\"])\n",
    "        feature_2.append(v[\"feature_2\"])\n",
    "        feature_3.append(v[\"feature_3\"])\n",
    "        feature_4.append(v[\"feature_4\"])\n",
    "        feature_5.append(v[\"feature_5\"])\n",
    "    meta = pd.DataFrame({\n",
    "        'element_uid': element_uid,\n",
    "        'duration': duration,\n",
    "        'type': type_,\n",
    "        # \"attributes\": attributes,\n",
    "        # \"availability\": attributes,\n",
    "        \"feature_1\": feature_1,\n",
    "        \"feature_2\": feature_2,\n",
    "        \"feature_3\": feature_3,\n",
    "        \"feature_4\": feature_4,\n",
    "        \"feature_5\": feature_5,\n",
    "    })\n",
    "    return meta\n",
    "\n",
    "meta = meta_to_df(meta_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normilize(feature:pd.Series) -> pd.Series:\n",
    "    return (feature - feature.mean())/feature.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions_with_meta = (\n",
    "    trainsactions\n",
    "    .merge(meta[[\n",
    "        'element_uid',\n",
    "        'duration',\n",
    "        'type',\n",
    "        \"feature_1\",\n",
    "        \"feature_2\",\n",
    "        \"feature_3\",\n",
    "        \"feature_4\",\n",
    "        \"feature_5\",\n",
    "    ]], on='element_uid', how='left')\n",
    ")\n",
    "\n",
    "trainsactions_with_meta[\"feature_1\"] = feature_normilize(\n",
    "    trainsactions_with_meta[\"feature_1\"])\n",
    "trainsactions_with_meta[\"feature_1\"] = feature_normilize(\n",
    "    trainsactions_with_meta[\"feature_2\"])\n",
    "trainsactions_with_meta[\"feature_3\"] = feature_normilize(\n",
    "    trainsactions_with_meta[\"feature_3\"])\n",
    "trainsactions_with_meta[\"feature_4\"] = feature_normilize(\n",
    "    trainsactions_with_meta[\"feature_4\"])\n",
    "trainsactions_with_meta[\"feature_5\"] = feature_normilize(\n",
    "    trainsactions_with_meta[\"feature_5\"])\n",
    "\n",
    "trainsactions_with_meta[\"main_feature\"] = trainsactions_with_meta[[\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"]].mean(axis=1).round()\n",
    "\n",
    "\n",
    "trainsactions_with_meta['watched_ratio'] = (\n",
    "    trainsactions_with_meta['watched_time'] /\n",
    "    trainsactions_with_meta['duration']\n",
    ")\n",
    "\n",
    "\n",
    "def score_transaction(t):\n",
    "    score = 1\n",
    "    if t['duration'] > 0:\n",
    "        if t['type'] == 'movie':\n",
    "            if t['watched_ratio'] > 0.3:\n",
    "                score = int(t['watched_ratio'] * 9) + 1\n",
    "        else:\n",
    "            if t['watched_ratio'] > 1:\n",
    "                score = int(t['watched_ratio'] / 2) + 1\n",
    "    if t['bookmark'] > 0 and t['watched_ratio'] <= 0.3:\n",
    "        score += 10\n",
    "    return min(10, max(int(score+t[\"main_feature\"]), 1))\n",
    "    # return min(10, score)\n",
    "\n",
    "\n",
    "trainsactions_with_meta['score'] = trainsactions_with_meta.apply(\n",
    "    score_transaction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions_with_meta_and_ratings = (\n",
    "    trainsactions_with_meta\n",
    "    .merge(\n",
    "        ratings[['user_uid', 'element_uid', 'rating']],\n",
    "        on=['user_uid', 'element_uid'],\n",
    "        how='outer'\n",
    "    )\n",
    ")\n",
    "\n",
    "trainsactions_with_meta_and_ratings['score'].fillna(\n",
    "    trainsactions_with_meta_and_ratings['rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsactions_with_meta_and_ratings.drop(index=trainsactions_with_meta_and_ratings[trainsactions_with_meta_and_ratings[\"duration\"].isna()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_tfidf_coo(transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    score_sum_per_user = (\n",
    "        transactions\n",
    "        .groupby('user_uid')['score']\n",
    "        .transform('sum')\n",
    "    )\n",
    "    user_count_per_element = (\n",
    "        transactions\n",
    "        .groupby('element_uid')['user_uid']\n",
    "        .transform('size')\n",
    "    )\n",
    "    tf = transactions['score'].values / score_sum_per_user.values\n",
    "    idf = np.log(len(score_sum_per_user) / user_count_per_element.values)\n",
    "    \n",
    "    tfidf = transactions[['user_uid', 'element_uid']].copy()\n",
    "    tfidf['value'] = tf * idf\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def encode_tfidf(transactions: pd.DataFrame) -> Tuple[LabelEncoder, LabelEncoder, csr_matrix]:\n",
    "    tfidf = encode_tfidf_coo(transactions)\n",
    "    \n",
    "    n_users = tfidf['user_uid'].nunique()\n",
    "    n_elements = tfidf['element_uid'].nunique()\n",
    "\n",
    "    user_encoder = LabelEncoder()\n",
    "    element_encoder = LabelEncoder()\n",
    "    user_index = user_encoder.fit_transform(transactions['user_uid'].values)\n",
    "    element_index = element_encoder.fit_transform(transactions['element_uid'].values)\n",
    "\n",
    "    tfidf_csr = csr_matrix(\n",
    "        (\n",
    "            tfidf['value'].astype('float32').values,\n",
    "            (user_index, element_index)\n",
    "        ),\n",
    "        shape=(n_users, n_elements)\n",
    "    )\n",
    "\n",
    "    return user_encoder, element_encoder, tfidf_csr\n",
    "\n",
    "\n",
    "user_encoder, element_encoder, transactions_csr = \\\n",
    "    encode_tfidf(trainsactions_with_meta_and_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:57<00:00, 12.58s/it, loss=0.0104]\n"
     ]
    }
   ],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "als = AlternatingLeastSquares(factors=128, iterations=100, alpha=40.0, calculate_training_loss=True)\n",
    "als.fit(transactions_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_matrix, recommendations_scores = als.recommend(\n",
    "    np.arange(0, transactions_csr.shape[0]), \n",
    "    transactions_csr, \n",
    "    N=10, \n",
    "    filter_already_liked_items=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_recommendations_to_df(\n",
    "    recommendations_matrix: np.ndarray, \n",
    "    recommendations_scores: np.ndarray,\n",
    "    user_encoder: LabelEncoder, \n",
    "    item_encoder: LabelEncoder,\n",
    "    user_key = 'user_id',\n",
    "    item_key = 'item_id'\n",
    ") -> pd.DataFrame:\n",
    "    recommendations_indices = pd.DataFrame({\n",
    "        'user_index': np.arange(0, len(recommendations_matrix)),\n",
    "        'item_index': list(recommendations_matrix),\n",
    "        'score': list(recommendations_scores),\n",
    "    })\n",
    "\n",
    "    user_mapping = pd.DataFrame({\n",
    "        'user_index': np.arange(0, len(user_encoder.classes_)),\n",
    "        user_key: user_encoder.classes_,\n",
    "    })\n",
    "\n",
    "    item_mapping = pd.DataFrame({\n",
    "        'item_index': np.arange(0, len(item_encoder.classes_)),\n",
    "        item_key: item_encoder.classes_,\n",
    "    })\n",
    "\n",
    "    recommendations = (\n",
    "        recommendations_indices\n",
    "        .merge(\n",
    "            user_mapping,\n",
    "            on='user_index',\n",
    "            how='left',\n",
    "        )\n",
    "        .drop(columns=['user_index'])\n",
    "        .explode(['item_index', 'score'], ignore_index=True)\n",
    "        .merge(\n",
    "            item_mapping,\n",
    "            on='item_index',\n",
    "            how='left',\n",
    "        )\n",
    "        .drop(columns=['item_index'])\n",
    "    )\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "recommendations = als_recommendations_to_df(\n",
    "    recommendations_matrix,\n",
    "    recommendations_scores,\n",
    "    user_encoder,\n",
    "    element_encoder,\n",
    "    user_key='user_uid',\n",
    "    item_key='element_uid',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = recommendations.groupby(\"user_uid\", as_index=False).agg({\"element_uid\":list}).rename(columns={\"element_uid\":\"recommended_element_uid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"recommended_element_uid\"] = [' '.join(map(str, l)) for l in result_df[\"recommended_element_uid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"my_recomendations.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f99b0d7b2032c78286f87aca85b3d6980edc424abfeeb901c0d39dce752615a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
